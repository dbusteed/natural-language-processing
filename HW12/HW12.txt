For this assignment I decided to use spaCy. I've always chosen to use NLTK for NLP tasks instead of spaCy, because I'm more familiar with NLTK. But since I've used NLTK and coreNLP in past tasks, I thought it would be fun to try a new tool. I really liked spaCy, especially for its easy-to-use documentation. It also had a built in visualizer called "displacy", which made displaying dependency diagrams super easy. SpaCy requires a model for processing, so I used the default model en_core_web_sm which was trained on the OntoNotes corpus.

I ran dependency parsing on the following texts and analyzed the results:

    1. Example sentence from class: "The boy put the tortoise on the rug." I wanted to make sure that my setup with spaCy was working correctly, so I first ran dependency parsing and this example sentence that was found in the class slides. SpaCy's output was very similar to the one given in the slides, so I moved on to more interesting texts.

    2. I parsed 1 Nephi 1:3, since I had the text file in reach from a previous assignment, and I was curious how it would handle sentence structures that might different than its language model. It was interesting to see a dependency diagram for a longer and more complex sentence. It was cool to see some of the longer relationships that the parser is able to pick up on. For example, in the parse tree seen in nephi.png, there is long "conj" relationship between "know" and "make". I'm still unsure on the true linguistic meaning of conjunctions even after reading the Wikipedia page, but it makes sense that these words are related, given that Nephi is saying that he KNOWs the record is true, and that he MAKEs it by his own hand.

    3. Next was a few famous lines from Hamlet, cause I wanted to see how the parser would handle poetic style. The first line of the text, seen in hamlet.png, seems to have a done a decent dependency parse. The following lines had some errors in the parse, partly because of spaCy's sentence splitting algorithm had issues with the poetic style, which I wasn't too surprised about. Because of this, other parse trees from the Hamlet doc seemed a little wonky cause they were given a fragment to interpret.
    
    4. Next, I did a headline and first line of a story from the New York Times. With the headline, it looks like the parser had issues with everything being capitalized. For example, it marked "Trump Orders Withdrawal" as one compound, rather than knowing that the correct relationship would have been "Orders --(nsubj)--> Trump". (see times_headline.png). The parser did fine with the actual body of the article. One example of this was the "appostional modifier" (another thing I had to look up on Wiki) between "withdrawal" and "decision". The parser is super smart.

    5. The last text I used was a negative review about BYU that I used from the sentiment task. I thought maybe the parser wouldn't perform as well with informal language, but it did really well. The review that was parsed was pretty straightforward, and didn't include any weird emojis and abbreviations that may have confused it. Instead, I found that these dependency trees were the easiest for me to understand, especially since I don't come from a linguistics background (see review.png) 

Overall, I think the spaCy dependency parse was very robust, and had great coverage and accuracy, especially for modern language.

The script I used to run everything can be found at https://github.com/dbusteed/nlp/tree/master/HW12.